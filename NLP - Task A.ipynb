{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Norbz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/Norbz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "import string\n",
    "import html\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "\n",
    "# KERAS / TF\n",
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Bidirectional, concatenate, \\\n",
    "                         CuDNNLSTM, CuDNNGRU, SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Input, \\\n",
    "                         Flatten, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "print(\"GPUs: \" + str(K.tensorflow_backend._get_available_gpus()))\n",
    "\n",
    "# Flags\n",
    "balance_dataset = False              # If true, it under-samples the training dataset to get same amount of labels\n",
    "use_pretrained_embeddings = True     # If true, it enables the use of GloVe pre-trained Twitter word-embeddings\n",
    "EDIT_DISTANCE =                      2 # Max edit distance for spelling correction suggestions. Setting this to 3 takes a lot longer to create the sym_spell dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.tensorflow_backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC AUC & macro-F1 score Printing Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class ROC_F1(Callback):\n",
    "    def __init__(self, validation_data=(), training_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.X_train, self.y_train = training_data\n",
    "        self.f1s_train = []\n",
    "        self.f1s_val = []\n",
    "        self.aucs_train = []\n",
    "        self.aucs_val = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred_train = np.round(self.model.predict(self.X_train, verbose=0))\n",
    "            y_pred_val   = np.round(self.model.predict(self.X_val, verbose=0))\n",
    "\n",
    "            auc_train = roc_auc_score(self.y_train, y_pred_train)\n",
    "            auc_val   = roc_auc_score(self.y_val, y_pred_val)\n",
    "            f1_train = f1_score(self.y_train, y_pred_train, average='macro')\n",
    "            f1_val   = f1_score(self.y_val, y_pred_val, average='macro')\n",
    "            \n",
    "            self.aucs_train.append(auc_train)\n",
    "            self.aucs_val.append(auc_val)\n",
    "            self.f1s_val.append(f1_val)\n",
    "            self.f1s_train.append(f1_train)\n",
    "            \n",
    "            print(\"     - LR: {:0.5f} train_auc: {:.4f} - train_F1: {:.4f} - val_auc: {:.4f} - val_F1: {:.4f}\".format(K.eval(self.model.optimizer.lr), auc_train, f1_train, auc_val, f1_val))\n",
    "        if epoch % 1 == 0:\n",
    "            height = 3.5;    width = height*4\n",
    "            plt.figure(figsize=(width,height))\n",
    "            plt.plot(self.f1s_train, label=\"Train F1\")\n",
    "            plt.plot(self.f1s_val, label=\"Validation F1\")\n",
    "            plt.xlim([0,50]); plt.xticks(list(range(50)));   plt.grid(True);   plt.legend()\n",
    "            plt.title(\"F1-score\", fontsize=15)\n",
    "            plt.show()\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tweet Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet,\n",
    "                  remove_USER_URL=True, \n",
    "                  remove_punctuation=True, \n",
    "                  remove_stopwords=True, \n",
    "                  remove_HTMLentities=True, \n",
    "                  remove_hashtags=True, \n",
    "                  appostrophe_handling=True, \n",
    "                  lemmatize=True, \n",
    "                  trial=False,\n",
    "                  sym_spell=None,\n",
    "                  reduce_lengthenings=True,\n",
    "                  segment_words=True,\n",
    "                  correct_spelling=True,\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    This function receives tweets and returns clean word-list\n",
    "    \"\"\"\n",
    "    ### Handle USERS and URLS ################################################\n",
    "    if remove_USER_URL:\n",
    "        if trial:\n",
    "            tweet = re.sub(r'@\\w+ ?', '', tweet)\n",
    "            tweet = re.sub(r'http\\S+', '', tweet)\n",
    "        else:\n",
    "            tweet = re.sub(r\"@USER\", \"<>\", tweet)\n",
    "            tweet = re.sub(r\"URL\", \"\", tweet)\n",
    "    else:\n",
    "        if trial:\n",
    "            tweet = re.sub(r'@\\w+ ?', '<usertoken> ', tweet)\n",
    "            tweet = re.sub(r'http\\S+', '<urltoken> ', tweet)\n",
    "        else:\n",
    "            tweet = re.sub(r\"@USER\", \"<usertoken>\", tweet)\n",
    "            tweet = re.sub(r\"URL\", \"<urltoken>\", tweet)\n",
    "    \n",
    "    ### Remove HTML Entities #################################################\n",
    "    if remove_HTMLentities:\n",
    "        tweet = html.unescape(tweet)\n",
    "    \n",
    "    ### REMOVE HASHTAGS? #####################################################\n",
    "    if remove_hashtags:\n",
    "        tweet = re.sub(r'#\\w+ ?', '', tweet)\n",
    "    \n",
    "    ### Convert to lower case: Hi->hi, MAGA -> maga ##########################\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    ### Cleaning: non-ASCII filtering, some appostrophes, separation #########\n",
    "    tweet = re.sub(r\"â€™\", r\"'\", tweet)\n",
    "    tweet = re.sub(r\"[^A-Za-z0-9'^,!.\\/+-=@]\", \" \", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is \", tweet)\n",
    "    tweet = re.sub(r\"\\'s\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\'ve\", \" have \", tweet)\n",
    "    tweet = re.sub(r\"n't\", \" not \", tweet)\n",
    "#     tweet = re.sub(r\"i'm\", \"i am \", tweet)\n",
    "    tweet = re.sub(r\"\\'re\", \" are \", tweet)\n",
    "    tweet = re.sub(r\"\\'d\", \" would \", tweet)\n",
    "    tweet = re.sub(r\"\\'ll\", \" will \", tweet)\n",
    "    tweet = re.sub(r\",\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\.\", \" \", tweet)\n",
    "    tweet = re.sub(r\"!\", \" ! \", tweet)\n",
    "    tweet = re.sub(r\"\\/\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\^\", \" ^ \", tweet)\n",
    "    tweet = re.sub(r\"\\+\", \" + \", tweet)\n",
    "    tweet = re.sub(r\"\\-\", \" - \", tweet)\n",
    "    tweet = re.sub(r\"\\=\", \" = \", tweet)\n",
    "    tweet = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", tweet)\n",
    "    tweet = re.sub(r\":\", \" : \", tweet)\n",
    "    tweet = re.sub(r\" e g \", \" eg \", tweet)\n",
    "    tweet = re.sub(r\" b g \", \" bg \", tweet)\n",
    "    tweet = re.sub(r\" u s \", \" american \", tweet)\n",
    "    tweet = re.sub(r\"\\0s\", \"0\", tweet)\n",
    "    tweet = re.sub(r\" 9 11 \", \"911\", tweet)\n",
    "    tweet = re.sub(r\"e - mail\", \"email\", tweet)\n",
    "    tweet = re.sub(r\"j k\", \"jk\", tweet)\n",
    "    tweet = re.sub(r\"\\s{2,}\", \" \", tweet)\n",
    "\n",
    "    ### Remove Punctuation ###################################################\n",
    "    if remove_punctuation:\n",
    "        translator = str.maketrans('', '', ''.join(list(set(string.punctuation) - set(\"'\"))))\n",
    "        tweet = tweet.translate(translator)\n",
    "    \n",
    "    # Tokenize sentence for further word-level processing\n",
    "    tokenizer  = TweetTokenizer()\n",
    "    words = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    ### Apostrophe handling:    you're   -> you are  ########################\n",
    "    APPO = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\", \"haven't\" : \"have not\", \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\", \"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\", \"isn't\" : \"is not\", \"it's\" : \"it is\", \"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\", \"mustn't\" : \"must not\", \"shan't\" : \"shall not\", \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \"shouldn't\" : \"should not\", \"that's\" : \"that is\", \"there's\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\", \"they're\" : \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\", \"we've\" : \"we have\", \"what'll\" : \"what will\", \"what're\" : \"what are\", \"what's\" : \"what is\", \"what've\" : \"what have\", \"where's\" : \"where is\", \"who'd\" : \"who would\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\" : \"would not\", \"you'd\" : \"you would\", \"you'll\" : \"you will\", \"you're\" : \"you are\", \"you've\" : \"you have\", \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\"}\n",
    "    if appostrophe_handling:\n",
    "        words = [APPO[word] if word in APPO else word for word in words]\n",
    "    \n",
    "    tweet = ' '.join(words)\n",
    "    words = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    ### Lemmatisation:          drinking -> drink ###########################\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word, \"v\") for word in words]\n",
    "    \n",
    "    ### Remove stop words:      is, that, the, ... ##########################\n",
    "    if remove_stopwords:\n",
    "        eng_stopwords = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in eng_stopwords]\n",
    "        \n",
    "    ### Reduce lengthening:    aaaaaaaaah -> aah, bleeeeerh -> bleerh #################\n",
    "    if reduce_lengthenings:\n",
    "        pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "        words = [pattern.sub(r\"\\1\\1\", w) for w in words]\n",
    "\n",
    "    if sym_spell and (segment_words or correct_spelling):\n",
    "\n",
    "        ### Segment words:    thecatonthemat -> the cat on the mat ####################\n",
    "        if segment_words:\n",
    "            words = [sym_spell.word_segmentation(word).corrected_string for word in words]\n",
    "\n",
    "        ### Correct spelling: birberals -> liberals ######################\n",
    "        if correct_spelling:\n",
    "            def correct_spelling_for_word(word):\n",
    "                suggestions = sym_spell.lookup(word, Verbosity.TOP, EDIT_DISTANCE)\n",
    "\n",
    "                if len(suggestions) > 0:\n",
    "                    return suggestions[0].term\n",
    "                return word\n",
    "            \n",
    "            words = [correct_spelling_for_word(word) for word in words]\n",
    "        \n",
    "    clean_tweet = \" \".join(words)\n",
    "    clean_tweet = re.sub(\"  \",\" \",clean_tweet)\n",
    "    clean_tweet = clean_tweet.lower()\n",
    "    \n",
    "    return clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def under_sample(X, y):\n",
    "    idx_0 = np.where(y==0)[0].tolist()\n",
    "    idx_1 = np.where(y==1)[0].tolist()\n",
    "\n",
    "    N = np.min([len(idx_0), len(idx_1)])\n",
    "    idx = idx_0[:N] + idx_1[:N]\n",
    "    shuffle(idx)\n",
    "    \n",
    "    X = X[idx].reshape(-1)\n",
    "    y = y[idx].reshape(-1,1)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LOAD DATA AND PRE-PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(remove_USER_URL=True,\n",
    "              remove_stopwords=False,\n",
    "              remove_HTMLentities=True,\n",
    "              remove_punctuation=True,\n",
    "              appostrophe_handling=True,\n",
    "              lemmatize=True,\n",
    "              reduce_lengthenings=True,\n",
    "              segment_words=False,\n",
    "              correct_spelling=False\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embedding index [word->vector]\n",
      " - Done! (45.06s)\n",
      "Loading training data\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "##   GloVe EMBEDDINGS     ##########################################################################\n",
    "####################################################################################################\n",
    "if use_pretrained_embeddings:\n",
    "    # Download embeddings from https://nlp.stanford.edu/projects/glove/\n",
    "    #                          https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "    embedding_path = \"glove.twitter.27B/glove.twitter.27B.100d.txt\"\n",
    "    embed_size     = 100\n",
    "    \n",
    "    def get_coefs(word,*arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    # Construct embedding table (word -> vector)\n",
    "    print(\"Building embedding index [word->vector]\", end=\"\\n\")\n",
    "    t0 = time.time()\n",
    "    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding=\"utf8\"))\n",
    "    print(\" - Done! ({:0.2f}s)\".format(time.time()-t0))\n",
    "    \n",
    "sym_spell = None\n",
    "if params['correct_spelling'] or params['segment_words']:\n",
    "    sym_spell = SymSpell(EDIT_DISTANCE, 7)\n",
    "    # For some reason, we have to write out the list of words contained in the embeddings out to a file for sym_spell to read from\n",
    "    print(\"Writing out corpus.txt\")\n",
    "    with open(\"corpus.txt\", \"w\") as f:\n",
    "        f.write(\" \".join(embedding_index.keys()))\n",
    "    print(\"Done!\")\n",
    "\n",
    "    print(\"Creating sym_spell dictionary\")\n",
    "    sym_spell.create_dictionary(\"corpus.txt\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "print(\"Loading training data\")\n",
    "df_a = pd.read_csv('start-kit/training-v1/offenseval-training-v1.tsv',sep='\\t')\n",
    "df_a_trial = pd.read_csv('start-kit/trial-data/offenseval-trial.txt', sep='\\t')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Done!\n",
      "EXAMPLES OF PROCESSED TWEETS [train/trial]\n",
      "_________________________________________________________________________________________________________\n",
      "Un-processed:  @USER She should ask a few native Americans what their take on this is.\n",
      "Processed:     she should ask a few native americans what their take on this be\n",
      "\n",
      "Un-processed:  @USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL\n",
      "Processed:     go home you be drink\n",
      "\n",
      "Un-processed:  Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT\n",
      "Processed:     amazon be investigate chinese employees who be sell internal data to third party sellers look for an edge in the competitive marketplace\n",
      "\n",
      "Un-processed:  @USER Someone should'veTaken\" this piece of shit to a volcano. ðŸ˜‚\"\n",
      "Processed:     someone should have take this piece of shit to a volcano\n",
      "\n",
      "_________________________________________________________________________________________________________\n",
      "Un-processed:  @BreitbartNews OK Shannon, YOU tell the veterans in those locker rooms they have to stay there until the celebration of what they fought for is over.\n",
      "Processed:     ok shannon you tell the veterans in those locker room they have to stay there until the celebration of what they fight for be over\n",
      "\n",
      "Un-processed:  @LeftyGlenn @jaredeker @BookUniverse @hashtagzema @RalphLombardi @NathanHRubin Fine... Because i could afford a gun if i wanted to. I could fit it into my budget. My budgeting is fine??? Here in canada we have gun insurance and gun control? And lotsa p\n",
      "Processed:     fine because i could afford a gun if i want to i could fit it into my budget my budget be fine here in canada we have gun insurance and gun control and lotsa p\n",
      "\n",
      "Un-processed:  Hot Mom Sucks Off Step Son In Shower 8 min https://t.co/Y0zi9f5z6J\n",
      "Processed:     hot mom suck off step son in shower 8 min\n",
      "\n",
      "Un-processed:  bro these are some cute butt plugs Iâ€™m trying to cop https://t.co/RsnxRF4HTi\n",
      "Processed:     bro these be some cute butt plug i be try to cop\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing...\")\n",
    "\n",
    "X = df_a['tweet'].apply(lambda x: process_tweet(x, **params, trial=False, sym_spell=sym_spell)).values;\n",
    "y = df_a['subtask_a'].replace({'OFF': 1, 'NOT': 0}).values;\n",
    "class_weights = sklearn.utils.class_weight.compute_class_weight('balanced', np.unique(y), y.reshape(-1))\n",
    "\n",
    "X_trial = df_a_trial['tweet'].apply(lambda x: process_tweet(x, **params, trial=True, sym_spell=sym_spell)).values;\n",
    "y_trial = df_a_trial['subtask_a'].replace({'OFF': 1, 'NOT': 0}).values;\n",
    "print(\"Done!\")\n",
    "\n",
    "if balance_dataset:\n",
    "    X, y = under_sample(X, y)\n",
    "\n",
    "\n",
    "print(\"EXAMPLES OF PROCESSED TWEETS [train/trial]\")\n",
    "print(\"_________________________________________________________________________________________________________\")\n",
    "for id in range(4):\n",
    "    print(\"Un-processed:  \" + df_a['tweet'][id])\n",
    "    print(\"Processed:     \" + X[id])\n",
    "    print(\"\")\n",
    "print(\"_________________________________________________________________________________________________________\")\n",
    "for id in range(4):\n",
    "    print(\"Un-processed:  \" + df_a_trial['tweet'][id])\n",
    "    print(\"Processed:     \" + X_trial[id])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabulary by Hand (Not of any actual use, only to print size and unique word count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This corpus contains 257435 tokens, of which 14477 are unique. I.e. avg of 19.44 words per tweet.\n"
     ]
    }
   ],
   "source": [
    "def get_tokenized_corpus(corpus):\n",
    "    tokenized_corpus = []\n",
    "    for sentence in corpus:\n",
    "        tokenized_sentence = []\n",
    "        for token in sentence.split(' '): \n",
    "            tokenized_sentence.append(token)\n",
    "        tokenized_corpus.append(tokenized_sentence)\n",
    "    return tokenized_corpus\n",
    "\n",
    "\n",
    "tokenized_corpus = get_tokenized_corpus(X)\n",
    "tokenized_corpus\n",
    "\n",
    "vocabulary = []\n",
    "token_count = 0\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        token_count += 1\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "            \n",
    "print(\"This corpus contains {} tokens, of which {} are unique. I.e. avg of {:0.2f} words per tweet.\".format(token_count, len(vocabulary), token_count/df_a.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Model\n",
    "#### Tokenize tweets  |  Turn into Index sequences  |  Pad sequences to max_length  |  (optional) Use word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GloVe Twitter word-embedding how to:\n",
    "1. Read embeddings and build a [word -> vector] dictionary\n",
    "2. Use tokenizer [word -> index] dictionary to iterate through all words in vocab\n",
    "3. For each word in vocab (iterate over [word -> index] tokenizer dictionary): \n",
    "    - Attempt to get word's embedding vector from GloVe Twitter [word -> vector]\n",
    "    - If word is in embedding's vocabulary: add the vector to the embedding matrix at the right index\n",
    "    - If not, do not do anything (respective vector will be [0, 0, 0, ..., 0]\n",
    "\n",
    "Embedding matrix will be of size (nb_words + 1, embedding_dim), this will be loaded into the embeddings layer later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dc11ac51cf29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Tokenize all tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mX_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "##   BUILD VOCABULARY FROM CORPUS   ################################################################\n",
    "####################################################################################################\n",
    "max_seq_len    = 50\n",
    "max_features   = 30000\n",
    "\n",
    "# Tokenize all tweets\n",
    "tokenizer = Tokenizer(lower=True, filters='', split=' ')\n",
    "X_all = list(X) + list(X_trial)\n",
    "tokenizer.fit_on_texts(X_all)\n",
    "print(f\"Num of unique tokens in tokenizer: {len(tokenizer.word_index)}\")\n",
    "\n",
    "# Get sequences for each dataset\n",
    "sequences       = tokenizer.texts_to_sequences(X)\n",
    "sequences_trial = tokenizer.texts_to_sequences(X_trial)\n",
    "\n",
    "# Pad sequences\n",
    "X       = pad_sequences(sequences, maxlen = max_seq_len)\n",
    "X_trial = pad_sequences(sequences_trial, maxlen = max_seq_len)\n",
    "\n",
    "# Reshape labels\n",
    "y       = y.reshape(-1,1)\n",
    "y_trial = y_trial.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embedding matrix (14738, 100) - Done!\n",
      "This vocabulary has 14737 unique tokens of which 12940 are in the embeddings and 1797 are not\n",
      "Words not in Glove: 1797\n"
     ]
    }
   ],
   "source": [
    "if use_pretrained_embeddings:\n",
    "    # Build Embedding Matrix\n",
    "    n_words_in_glove = 0\n",
    "    n_words_not_in_glove = 0\n",
    "    words_in_glove = []\n",
    "    words_not_in_glove = []\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "    print(f\"Building embedding matrix {embedding_matrix.shape}\", end=\"\")\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            n_words_in_glove += 1\n",
    "            words_in_glove.append(word)\n",
    "        else:\n",
    "            n_words_not_in_glove += 1\n",
    "            words_not_in_glove.append(word)\n",
    "    print(\" - Done!\")\n",
    "    print(\"This vocabulary has {} unique tokens of which {} are in the embeddings and {} are not\".format(len(word_index), n_words_in_glove, n_words_not_in_glove))\n",
    "    print(f\"Words not in Glove: {len(words_not_in_glove)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0acb74225437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean sentence length: {:0.1f} words\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MAX  sentence length: {} words\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sequences' is not defined"
     ]
    }
   ],
   "source": [
    "sentence_lengths = [len(tokens) for tokens in sequences]\n",
    "print(\"Mean sentence length: {:0.1f} words\".format(np.mean(sentence_lengths)))\n",
    "print(\"MAX  sentence length: {} words\".format(np.max(sentence_lengths)))\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5)) \n",
    "plt.xlabel('Tweet length', fontsize=15)\n",
    "plt.ylabel('Number of Tweets', fontsize=15)\n",
    "plt.hist(sentence_lengths, bins=list(range(70)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BUILD MODEL & TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define RNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Bi_GRU_LSTM_CN_model(lr=0.001, lr_decay=0.01, recurrent_units=0, dropout=0.0):\n",
    "    # Model architecture\n",
    "    inputs = Input(shape = (max_seq_len,), name=\"Input\")\n",
    "    \n",
    "    emb = Embedding(nb_words+1, embed_size, trainable=train_embeddings, name=\"WordEmbeddings\")(inputs)\n",
    "    emb = SpatialDropout1D(dropout)(emb)\n",
    "\n",
    "    gru_out  = Bidirectional(CuDNNGRU(RECURRENT_UNITS, return_sequences = True), name=\"Bi_GRU\")(emb)\n",
    "    gru_out  = Conv1D(32, 4, activation='relu', padding='valid', kernel_initializer='he_uniform')(gru_out)\n",
    "\n",
    "    lstm_out = Bidirectional(CuDNNLSTM(RECURRENT_UNITS, return_sequences = True), name=\"Bi_LSTM\")(emb)\n",
    "    lstm_out = Conv1D(32, 4, activation='relu', padding='valid', kernel_initializer='he_uniform')(lstm_out)\n",
    "\n",
    "    avg_pool1 = GlobalAveragePooling1D(name=\"GlobalAVGPooling_GRU\")(gru_out)\n",
    "    max_pool1 = GlobalMaxPooling1D(name=\"GlobalMAXPooling_GRU\")(gru_out)\n",
    "\n",
    "    avg_pool2 = GlobalAveragePooling1D(name=\"GlobalAVGPooling_LSTM\")(lstm_out)\n",
    "    max_pool2 = GlobalMaxPooling1D(name=\"GlobalMAXPooling_LSTM\")(lstm_out) \n",
    "\n",
    "    x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "\n",
    "    outputs = Dense(1, activation='sigmoid', name=\"Output\")(x)\n",
    "    \n",
    "    model = Model(inputs,outputs)\n",
    "    \n",
    "    return model, 1\n",
    "\n",
    "def build_LSTM():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words+1, embed_size, input_length=max_seq_len, trainable=train_embeddings, name=\"Embeddings\"))\n",
    "    model.add(SpatialDropout1D(DROPOUT))\n",
    "    model.add(Bidirectional(CuDNNLSTM(RECURRENT_UNITS)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model, 0\n",
    "\n",
    "def build_CNN_LSTM():\n",
    "    EMBEDDING_DIM = embed_size\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words+1, EMBEDDING_DIM, input_length=max_seq_len, trainable=train_embeddings, name=\"Embeddings\"))\n",
    "    model.add(SpatialDropout1D(DROPOUT))\n",
    "    model.add(Conv1D(64, 4, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(Bidirectional(LSTM(RECURRENT_UNITS, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model, 0\n",
    "\n",
    "def build_LSTM_CNN():\n",
    "    EMBEDDING_DIM = embed_size\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words+1, EMBEDDING_DIM, input_length=max_seq_len, trainable=train_embeddings, name=\"Embeddings\"))\n",
    "#     model.add(SpatialDropout1D(DROPOUT))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Bidirectional(CuDNNLSTM(RECURRENT_UNITS, return_sequences=True)))\n",
    "#     model.add(Bidirectional(LSTM(EMBEDDING_DIM, return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)))\n",
    "    model.add(Conv1D(64, kernel_size=2, activation='relu', padding='valid', kernel_initializer='he_uniform'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# SET HYPERPARAMETERS -------------------------------------------------------------------------------------------\n",
    "LR               = 0.004\n",
    "LR_DECAY         = 0\n",
    "EPOCHS           = 20\n",
    "BATCH_SIZE       = 32\n",
    "EMBEDDING_DIM    = embed_size\n",
    "DROPOUT          = 0.4         # Connection drop ratio for CNN to LSTM dropout\n",
    "LSTM_DROPOUT     = 0.0         # Connection drop ratio for gate-specific dropout\n",
    "BIDIRECTIONAL    = True\n",
    "RECURRENT_UNITS  = 100\n",
    "train_embeddings = not use_pretrained_embeddings\n",
    "print([LR, LR_DECAY, EPOCHS, BATCH_SIZE, EMBEDDING_DIM, DROPOUT, LSTM_DROPOUT])\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# BUILD MODEL --------------------------------------------------------------------------------------------------\n",
    "# - Can select which architecture to use (simple LSTM works pretty well)\n",
    "model, embed_idx = build_LSTM()\n",
    "# model, embed_idx = build_CNN_LSTM()\n",
    "# model, embed_idx = build_LSTM_CNN()\n",
    "# model, embed_idx = build_Bi_GRU_LSTM_CN_model(LR, LR_DECAY, RECURRENT_UNITS, DROPOUT)\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# OPTIMIZER | COMPILE | EMBEDDINGS -----------------------------------------------------------------------------\n",
    "optim = optimizers.Adam(lr=LR, decay=LR_DECAY)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "if use_pretrained_embeddings:\n",
    "    model.layers[embed_idx].set_weights([embedding_matrix])\n",
    "model.summary()\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "# X_train, X_val, y_train, y_val = (X, y, X_trial, Y_trial)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Can change right hand side to (X, y, X_trial, y_trial) or (X_train, Y_train, X_val, Y_val)\n",
    "X_train, y_train, X_val, y_val = (X, y, X_trial, y_trial)\n",
    "\n",
    "class_weights = sklearn.utils.class_weight.compute_class_weight('balanced', np.unique(y_train), y_train.reshape(-1))\n",
    "weights_dict = dict()\n",
    "for i, weight in enumerate(class_weights):\n",
    "    weights_dict[i] = weight\n",
    "print(\"Class weights (to address dataset imbalance):\")\n",
    "display(weights_dict)\n",
    "\n",
    "\n",
    "# FIT THE MODEL ------------------------------------------------------------------------------------------------\n",
    "auc_f1     = ROC_F1(validation_data=(X_val, y_val), training_data=(X_train, y_train), interval=1)\n",
    "earlystop  = EarlyStopping(monitor='val_loss', patience=15, verbose=1, mode='auto', restore_best_weights=True)\n",
    "filepath   = \"weights-improvement-{epoch:02d}-{val_acc:.5f}-{val_loss:.5f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, save_best_only=True, monitor='val_loss', verbose=1, mode='min')\n",
    "\n",
    "train_history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                          verbose=1, callbacks=[earlystop, checkpoint, auc_f1])\n",
    "#---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate [Training History, Classification Report + Metrics]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "height = 3.5;    width = height*4\n",
    "n_epochs = 40\n",
    "# n_epochs = len(train_history.history['loss'])\n",
    "\n",
    "# # Plot Loss\n",
    "# plt.figure(figsize=(width,height))\n",
    "# plt.plot(train_history.history['loss'], label=\"Train Loss\")\n",
    "# plt.plot(train_history.history['val_loss'], label=\"Validation Loss\")\n",
    "# plt.xlim([0,n_epochs-1]); plt.xticks(list(range(n_epochs)));   plt.grid(True);   plt.legend()\n",
    "# plt.title(\"Loss (Binary Cross-entropy)\", fontsize=15)\n",
    "# plt.show()\n",
    "\n",
    "# # Plot accuracy\n",
    "# plt.figure(figsize=(width,height))\n",
    "# plt.plot(train_history.history['acc'], label=\"Train Accuracy\")\n",
    "# plt.plot(train_history.history['val_acc'], label=\"Validation Accuracy\")\n",
    "# plt.xlim([0,n_epochs-1]); plt.xticks(list(range(n_epochs)));   plt.grid(True);   plt.legend()\n",
    "# plt.title(\"Accuracy\", fontsize=15)\n",
    "# plt.show()\n",
    "\n",
    "# Plot F1\n",
    "plt.figure(figsize=(width,height))\n",
    "plt.plot(auc_f1.f1s_train, label=\"Train F1\")\n",
    "plt.plot(auc_f1.f1s_val, label=\"Validation F1\")\n",
    "plt.xlim([0,n_epochs-1]); plt.xticks(list(range(n_epochs)));   plt.grid(True);   plt.legend()\n",
    "plt.title(\"F1-score\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "# # Plot ROC AUC\n",
    "plt.figure(figsize=(width,height))\n",
    "plt.plot(auc_f1.aucs_train, label=\"Train ROC AUC\")\n",
    "plt.plot(auc_f1.aucs_val, label=\"Validation ROC AUC\")\n",
    "plt.xlim([0,n_epochs-1]); plt.xticks(list(range(n_epochs)));   plt.grid(True);   plt.legend()\n",
    "plt.legend()\n",
    "plt.title(\"ROC AUC\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "##### Confusion matrix & Classication Report ###################################################\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# def plot_confusion_matrix(cm, classes,\n",
    "#                           normalize=False,\n",
    "#                           title='Confusion matrix',\n",
    "#                           cmap=plt.cm.winter):\n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title, fontsize=30)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, fontsize=20)\n",
    "#     plt.yticks(tick_marks, classes, fontsize=20)\n",
    "    \n",
    "#     fmt = '.2f' if normalize else 'd'\n",
    "#     thresh = cm.max() / 2.\n",
    "\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "#                  color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label', fontsize=30)\n",
    "#     plt.xlabel('Predicted label', fontsize=30)\n",
    "\n",
    "#     return plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "X_eval = X_trial\n",
    "y_eval = y_trial\n",
    "\n",
    "y_pred = model.predict(X_eval)\n",
    "y_pred = np.round(y_pred)\n",
    "print(\"Validation Accuracy: {:0.2f}%\".format(np.sum(y_eval==y_pred)/y_eval.shape[0]*100))\n",
    "\n",
    "cm = confusion_matrix(y_eval, y_pred)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm, classes=['NOT-OFFENSIVE','OFFENSIVE'], normalize=True, title='Confusion matrix')\n",
    "plt.show()\n",
    "# print(cm)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_A.h5\")\n",
    "params = dict(remove_USER_URL=True,\n",
    "              remove_stopwords=False,\n",
    "              remove_HTMLentities=True,\n",
    "              remove_punctuation=True,\n",
    "              appostrophe_handling=True,\n",
    "              lemmatize=True)\n",
    "\n",
    "df_test = pd.read_csv('../OffensEval_data/test/testset-taska.tsv',sep='\\t')\n",
    "ID = df_test['id']\n",
    "\n",
    "X_test = df_test['tweet'].apply(lambda x: process_tweet(x, **params, trial=False)).values\n",
    "\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(sequences_test, maxlen = max_seq_len)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "OFF = y_pred\n",
    "NOT = 1-y_pred\n",
    "Y_pred = np.hstack((NOT, OFF))\n",
    "\n",
    "classes = np.argmax(Y_pred, axis=1).reshape(-1,1)\n",
    "\n",
    "df_submission = pd.DataFrame()\n",
    "df_submission['id'] = ID\n",
    "df_submission['label'] = classes\n",
    "df_submission['label'] = df_submission['label'].replace({0: 'NOT', 1: 'OFF'})\n",
    "\n",
    "df_submission.head()\n",
    "df_submission.to_csv(\"submission_a/test_a_submission.csv\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO USE TWITTER GLOVE EMBEDDINGS ##########################\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "# https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "\n",
    "# PRE-PROCESS\n",
    "import re\n",
    "def glove_preprocess(text):\n",
    "    \"\"\"\n",
    "    adapted from https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n",
    "    \"\"\"\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = \"[8:=;]\"\n",
    "    nose = \"['`\\-]?\"\n",
    "    text = re.sub(r'http\\S+', '<URL> ', text)\n",
    "    text = re.sub(r'@\\S+', '<USER>', text)\n",
    "    text = re.sub(\"www.* \", \"<URL>\", text)\n",
    "    text = re.sub(\"\\[\\[User(.*)\\|\", '<USER>', text)\n",
    "    text = re.sub(\"<3\", r'<HEART> ', text)\n",
    "    text = re.sub(\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<NUMBER>\", text)\n",
    "    text = re.sub(eyes + nose + \"[Dd)]\", '<SMILE>', text)\n",
    "    text = re.sub(\"[(d]\" + nose + eyes, '<SMILE>', text)\n",
    "    text = re.sub(eyes + nose + \"p\", '<LOLFACE>', text)\n",
    "    text = re.sub(eyes + nose + \"\\(\", '<SADFACE>', text)\n",
    "    text = re.sub(\"\\)\" + nose + eyes, '<SADFACE>', text)\n",
    "    text = re.sub(eyes + nose + \"[/|l*]\", '<NEUTRALFACE>', text)\n",
    "    text = re.sub(\"/\", \" / \", text)\n",
    "    text = re.sub(\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<NUMBER>\", text)\n",
    "    text = re.sub(\"([!]){2,}\", \"! <REPEAT>\", text)\n",
    "    text = re.sub(\"([?]){2,}\", \"? <REPEAT>\", text)\n",
    "    text = re.sub(\"([.]){2,}\", \". <REPEAT>\", text)\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    text = pattern.sub(r\"\\1\" + \" <ELONG>\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"@handermoren you are an absolute moron ???????? idiot 6 :( :p :) https://google.es\"\n",
    "print(text)\n",
    "text = glove_preprocess(text)\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "482px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
