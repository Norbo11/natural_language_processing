{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Norbz/anaconda/envs/natural_language_processing/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "[nltk_data] Downloading package wordnet to /Users/Norbz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/Norbz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "import string\n",
    "import html\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "\n",
    "# KERAS / TF\n",
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Bidirectional, concatenate, \\\n",
    "                         CuDNNLSTM, CuDNNGRU, SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Input, \\\n",
    "                         Flatten, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "print(\"GPUs: \" + str(K.tensorflow_backend._get_available_gpus()))\n",
    "\n",
    "# Flags\n",
    "balance_dataset = False              # If true, it under-samples the training dataset to get same amount of labels\n",
    "use_pretrained_embeddings = True     # If true, it enables the use of GloVe pre-trained Twitter word-embeddings\n",
    "EDIT_DISTANCE =                      2 # Max edit distance for spelling correction suggestions. Setting this to 3 takes a lot longer to create the sym_spell dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC AUC & macro-F1 score Printing Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class ROC_F1(Callback):\n",
    "    def __init__(self, validation_data=(), training_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.X_train, self.y_train = training_data\n",
    "        self.f1s_train = []\n",
    "        self.f1s_val = []\n",
    "        self.aucs_train = []\n",
    "        self.aucs_val = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred_train = np.round(self.model.predict(self.X_train, verbose=0))\n",
    "            y_pred_val   = np.round(self.model.predict(self.X_val, verbose=0))\n",
    "\n",
    "            auc_train = roc_auc_score(self.y_train, y_pred_train)\n",
    "            auc_val   = roc_auc_score(self.y_val, y_pred_val)\n",
    "            f1_train = f1_score(self.y_train, y_pred_train, average='macro')\n",
    "            f1_val   = f1_score(self.y_val, y_pred_val, average='macro')\n",
    "            \n",
    "            self.aucs_train.append(auc_train)\n",
    "            self.aucs_val.append(auc_val)\n",
    "            self.f1s_val.append(f1_val)\n",
    "            self.f1s_train.append(f1_train)\n",
    "            \n",
    "            print(\"     - LR: {:0.5f} train_auc: {:.4f} - train_F1: {:.4f} - val_auc: {:.4f} - val_F1: {:.4f}\".format(K.eval(self.model.optimizer.lr), auc_train, f1_train, auc_val, f1_val))\n",
    "        if epoch % 1 == 0:\n",
    "            height = 3.5;    width = height*4\n",
    "            plt.figure(figsize=(width,height))\n",
    "            plt.plot(self.f1s_train, label=\"Train F1\")\n",
    "            plt.plot(self.f1s_val, label=\"Validation F1\")\n",
    "            plt.xlim([0,50]); plt.xticks(list(range(50)));   plt.grid(True);   plt.legend()\n",
    "            plt.title(\"F1-score\", fontsize=15)\n",
    "            plt.show()\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tweet Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_tweet(tweet,\n",
    "                  remove_USER_URL=True, \n",
    "                  remove_punctuation=True, \n",
    "                  remove_stopwords=True, \n",
    "                  remove_HTMLentities=True, \n",
    "                  remove_hashtags=True, \n",
    "                  appostrophe_handling=True, \n",
    "                  lemmatize=True, \n",
    "                  trial=False,\n",
    "                  sym_spell=None,\n",
    "                  reduce_lengthenings=True,\n",
    "                  segment_words=True,\n",
    "                  correct_spelling=True,\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    This function receives tweets and returns clean word-list\n",
    "    \"\"\"\n",
    "    ### Handle USERS and URLS ################################################\n",
    "    if remove_USER_URL:\n",
    "        if trial:\n",
    "            tweet = re.sub(r'@\\w+ ?', '', tweet)\n",
    "            tweet = re.sub(r'http\\S+', '', tweet)\n",
    "        else:\n",
    "            tweet = re.sub(r\"@USER\", \"<>\", tweet)\n",
    "            tweet = re.sub(r\"URL\", \"\", tweet)\n",
    "    else:\n",
    "        if trial:\n",
    "            tweet = re.sub(r'@\\w+ ?', '<usertoken> ', tweet)\n",
    "            tweet = re.sub(r'http\\S+', '<urltoken> ', tweet)\n",
    "        else:\n",
    "            tweet = re.sub(r\"@USER\", \"<usertoken>\", tweet)\n",
    "            tweet = re.sub(r\"URL\", \"<urltoken>\", tweet)\n",
    "    \n",
    "    ### Remove HTML Entities #################################################\n",
    "    if remove_HTMLentities:\n",
    "        tweet = html.unescape(tweet)\n",
    "    \n",
    "    ### REMOVE HASHTAGS? #####################################################\n",
    "    if remove_hashtags:\n",
    "        tweet = re.sub(r'#\\w+ ?', '', tweet)\n",
    "    \n",
    "    ### Convert to lower case: Hi->hi, MAGA -> maga ##########################\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    ### Cleaning: non-ASCII filtering, some appostrophes, separation #########\n",
    "    tweet = re.sub(r\"’\", r\"'\", tweet)\n",
    "    tweet = re.sub(r\"[^A-Za-z0-9'^,!.\\/+-=@]\", \" \", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is \", tweet)\n",
    "    tweet = re.sub(r\"\\'s\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\'ve\", \" have \", tweet)\n",
    "    tweet = re.sub(r\"n't\", \" not \", tweet)\n",
    "#     tweet = re.sub(r\"i'm\", \"i am \", tweet)\n",
    "    tweet = re.sub(r\"\\'re\", \" are \", tweet)\n",
    "    tweet = re.sub(r\"\\'d\", \" would \", tweet)\n",
    "    tweet = re.sub(r\"\\'ll\", \" will \", tweet)\n",
    "    tweet = re.sub(r\",\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\.\", \" \", tweet)\n",
    "    tweet = re.sub(r\"!\", \" ! \", tweet)\n",
    "    tweet = re.sub(r\"\\/\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\^\", \" ^ \", tweet)\n",
    "    tweet = re.sub(r\"\\+\", \" + \", tweet)\n",
    "    tweet = re.sub(r\"\\-\", \" - \", tweet)\n",
    "    tweet = re.sub(r\"\\=\", \" = \", tweet)\n",
    "    tweet = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", tweet)\n",
    "    tweet = re.sub(r\":\", \" : \", tweet)\n",
    "    tweet = re.sub(r\" e g \", \" eg \", tweet)\n",
    "    tweet = re.sub(r\" b g \", \" bg \", tweet)\n",
    "    tweet = re.sub(r\" u s \", \" american \", tweet)\n",
    "    tweet = re.sub(r\"\\0s\", \"0\", tweet)\n",
    "    tweet = re.sub(r\" 9 11 \", \"911\", tweet)\n",
    "    tweet = re.sub(r\"e - mail\", \"email\", tweet)\n",
    "    tweet = re.sub(r\"j k\", \"jk\", tweet)\n",
    "    tweet = re.sub(r\"\\s{2,}\", \" \", tweet)\n",
    "\n",
    "    ### Remove Punctuation ###################################################\n",
    "    if remove_punctuation:\n",
    "        translator = str.maketrans('', '', ''.join(list(set(string.punctuation) - set(\"'\"))))\n",
    "        tweet = tweet.translate(translator)\n",
    "    \n",
    "    # Tokenize sentence for further word-level processing\n",
    "    tokenizer  = TweetTokenizer()\n",
    "    words = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    ### Apostrophe handling:    you're   -> you are  ########################\n",
    "    APPO = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\", \"haven't\" : \"have not\", \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\", \"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\", \"isn't\" : \"is not\", \"it's\" : \"it is\", \"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\", \"mustn't\" : \"must not\", \"shan't\" : \"shall not\", \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \"shouldn't\" : \"should not\", \"that's\" : \"that is\", \"there's\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\", \"they're\" : \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\", \"we've\" : \"we have\", \"what'll\" : \"what will\", \"what're\" : \"what are\", \"what's\" : \"what is\", \"what've\" : \"what have\", \"where's\" : \"where is\", \"who'd\" : \"who would\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\" : \"would not\", \"you'd\" : \"you would\", \"you'll\" : \"you will\", \"you're\" : \"you are\", \"you've\" : \"you have\", \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\"}\n",
    "    if appostrophe_handling:\n",
    "        words = [APPO[word] if word in APPO else word for word in words]\n",
    "    \n",
    "    tweet = ' '.join(words)\n",
    "    words = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    ### Lemmatisation:          drinking -> drink ###########################\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word, \"v\") for word in words]\n",
    "    \n",
    "    ### Remove stop words:      is, that, the, ... ##########################\n",
    "    if remove_stopwords:\n",
    "        eng_stopwords = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in eng_stopwords]\n",
    "        \n",
    "    ### Reduce lengthening:    aaaaaaaaah -> aah, bleeeeerh -> bleerh #################\n",
    "    if reduce_lengthenings:\n",
    "        pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "        words = [pattern.sub(r\"\\1\\1\", w) for w in words]\n",
    "\n",
    "    if sym_spell and (segment_words or correct_spelling):\n",
    "\n",
    "        ### Segment words:    thecatonthemat -> the cat on the mat ####################\n",
    "        if segment_words:\n",
    "            words = [sym_spell.word_segmentation(word).corrected_string for word in words]\n",
    "\n",
    "        ### Correct spelling: birberals -> liberals ######################\n",
    "        if correct_spelling:\n",
    "            def correct_spelling_for_word(word):\n",
    "                suggestions = sym_spell.lookup(word, Verbosity.TOP, EDIT_DISTANCE)\n",
    "\n",
    "                if len(suggestions) > 0:\n",
    "                    return suggestions[0].term\n",
    "                return word\n",
    "            \n",
    "            words = [correct_spelling_for_word(word) for word in words]\n",
    "        \n",
    "    clean_tweet = \" \".join(words)\n",
    "    clean_tweet = re.sub(\"  \",\" \",clean_tweet)\n",
    "    clean_tweet = clean_tweet.lower()\n",
    "    \n",
    "    return clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def under_sample(X, y):\n",
    "    idx_0 = np.where(y==0)[0].tolist()\n",
    "    idx_1 = np.where(y==1)[0].tolist()\n",
    "\n",
    "    N = np.min([len(idx_0), len(idx_1)])\n",
    "    idx = idx_0[:N] + idx_1[:N]\n",
    "    shuffle(idx)\n",
    "    \n",
    "    X = X[idx].reshape(-1)\n",
    "    y = y[idx].reshape(-1,1)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LOAD DATA AND PRE-PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = dict(remove_USER_URL=True,\n",
    "              remove_stopwords=False,\n",
    "              remove_HTMLentities=True,\n",
    "              remove_punctuation=True,\n",
    "              appostrophe_handling=True,\n",
    "              lemmatize=True,\n",
    "              reduce_lengthenings=True,\n",
    "              segment_words=False,\n",
    "              correct_spelling=False\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "##   GloVe EMBEDDINGS     ##########################################################################\n",
    "####################################################################################################\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "embed_size     = 100\n",
    "\n",
    "if use_pretrained_embeddings:\n",
    "    path = Path(\"embedding_index.pkl\")\n",
    "    if path.is_file():\n",
    "        with open(\"embedding_index.pkl\", \"rb\") as f:\n",
    "            embedding_index = pickle.load(f)\n",
    "    else:\n",
    "        # Download embeddings from https://nlp.stanford.edu/projects/glove/\n",
    "        #                          https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "        embedding_path = \"glove.twitter.27B/glove.twitter.27B.100d.txt\"\n",
    "\n",
    "        def get_coefs(word,*arr):\n",
    "            return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "        # Construct embedding table (word -> vector)\n",
    "        print(\"Building embedding index [word->vector]\", end=\"\\n\")\n",
    "        t0 = time.time()\n",
    "        embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding=\"utf8\"))\n",
    "        \n",
    "        with open(\"embedding_index.pkl\", \"wb\") as f:\n",
    "            pickle.dump(embedding_index, f)\n",
    "            \n",
    "        print(\" - Done! ({:0.2f}s)\".format(time.time()-t0))\n",
    "    \n",
    "sym_spell = None\n",
    "# if params['correct_spelling'] or params['segment_words']:\n",
    "#     sym_spell = SymSpell(EDIT_DISTANCE, 7)\n",
    "#     # For some reason, we have to write out the list of words contained in the embeddings out to a file for sym_spell to read from\n",
    "#     print(\"Writing out corpus.txt\")\n",
    "#     with open(\"corpus.txt\", \"w\") as f:\n",
    "#         f.write(\" \".join(embedding_index.keys()))\n",
    "#     print(\"Done!\")\n",
    "\n",
    "#     print(\"Creating sym_spell dictionary\")\n",
    "#     sym_spell.create_dictionary(\"corpus.txt\")\n",
    "#     print(\"Done!\")\n",
    "\n",
    "print(\"Loading training data\")\n",
    "df_a = pd.read_csv('start-kit/training-v1/offenseval-training-v1.tsv',sep='\\t')\n",
    "df_a_trial = pd.read_csv('start-kit/trial-data/offenseval-trial.txt', sep='\\t')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Done!\n",
      "EXAMPLES OF PROCESSED TWEETS [train/trial]\n",
      "_________________________________________________________________________________________________________\n",
      "Un-processed:  @USER @USER @USER It’s not my fault you support gun control\n",
      "Processed:     it not my fault you support gun control\n",
      "\n",
      "Un-processed:  @USER What’s the difference between #Kavanaugh and @USER   One of these men admitted to groping a 15 year old girl years ago.  The other is going to be #confirmed to the SCJ   #DemsareFrauds #DemsAreDone   #WalkAwayDemocrats2018 #redwave #VoteRedSaveAmerica #trumptrain #MAGA URL\n",
      "Processed:     what be the difference between and one of these men admit to grope a 15 year old girl years ago the other be go to be to the scj\n",
      "\n",
      "Un-processed:  @USER you are a lying corrupt traitor!!! Nobody wants to hear anymore of your lies!!! #DeepStateCorruption URL\n",
      "Processed:     you be a lie corrupt traitor nobody want to hear anymore of your lie\n",
      "\n",
      "Un-processed:  @USER @USER @USER It should scare every American!  She is playing Hockey with a warped puck!\n",
      "Processed:     it should scare every american she be play hockey with a warp puck\n",
      "\n",
      "Un-processed:  @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER I like my soda like I like my boarders with a lot of ICE.\n",
      "Processed:     i like my soda like i like my boarders with a lot of ice\n",
      "\n",
      "_________________________________________________________________________________________________________\n",
      "Un-processed:  @NewYorker On the condition of self-reading after the completion of the marriage ceremony in the royal ship and the… https://t.co/67CptVIuYW\n",
      "Processed:     on the condition of self read after the completion of the marriage ceremony in the royal ship and the\n",
      "\n",
      "Un-processed:  Surprise Vote in Congress Protects Medical Marijuana From Federal Interference https://t.co/qtKKklkdke via @wordpressdotcom\n",
      "Processed:     surprise vote in congress protect medical marijuana from federal interference via\n",
      "\n",
      "Un-processed:  Nationwide support for gun control falls https://t.co/Q2ojIuFt7p https://t.co/BfWSSuFWMQ\n",
      "Processed:     nationwide support for gun control fall\n",
      "\n",
      "Un-processed:  @NOA2iCY ooooh i love pineapples. i’m hoping to find a piña colada in a pineapple\n",
      "Processed:     ooh i love pineapples i be hop to find a pi a colada in a pineapple\n",
      "\n",
      "Un-processed:  Columbia Care expands home delivery service to all patients in New York State. https://t.co/1tGLhpK98N\n",
      "Processed:     columbia care expand home delivery service to all patients in new york state\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing...\")\n",
    "\n",
    "X = df_a['tweet'].apply(lambda x: process_tweet(x, **params, trial=False, sym_spell=sym_spell)).values;\n",
    "y = df_a['subtask_a'].replace({'OFF': 1, 'NOT': 0}).values;\n",
    "class_weights = sklearn.utils.class_weight.compute_class_weight('balanced', np.unique(y), y.reshape(-1))\n",
    "\n",
    "X_trial = df_a_trial['tweet'].apply(lambda x: process_tweet(x, **params, trial=True, sym_spell=sym_spell)).values;\n",
    "y_trial = df_a_trial['subtask_a'].replace({'OFF': 1, 'NOT': 0}).values;\n",
    "print(\"Done!\")\n",
    "\n",
    "if balance_dataset:\n",
    "    X, y = under_sample(X, y)\n",
    "\n",
    "\n",
    "print(\"EXAMPLES OF PROCESSED TWEETS [train/trial]\")\n",
    "print(\"_________________________________________________________________________________________________________\")\n",
    "for id in range(10, 15):\n",
    "    print(\"Un-processed:  \" + df_a['tweet'][id])\n",
    "    print(\"Processed:     \" + X[id])\n",
    "    print(\"\")\n",
    "print(\"_________________________________________________________________________________________________________\")\n",
    "for id in range(10, 15):\n",
    "    print(\"Un-processed:  \" + df_a_trial['tweet'][id])\n",
    "    print(\"Processed:     \" + X_trial[id])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabulary by Hand (Not of any actual use, only to print size and unique word count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This corpus contains 257435 tokens, of which 14477 are unique. I.e. avg of 19.44 words per tweet.\n"
     ]
    }
   ],
   "source": [
    "def get_tokenized_corpus(corpus):\n",
    "    tokenized_corpus = []\n",
    "    for sentence in corpus:\n",
    "        tokenized_sentence = []\n",
    "        for token in sentence.split(' '): \n",
    "            tokenized_sentence.append(token)\n",
    "        tokenized_corpus.append(tokenized_sentence)\n",
    "    return tokenized_corpus\n",
    "\n",
    "\n",
    "tokenized_corpus = get_tokenized_corpus(X)\n",
    "tokenized_corpus\n",
    "\n",
    "vocabulary = []\n",
    "token_count = 0\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        token_count += 1\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "            \n",
    "print(\"This corpus contains {} tokens, of which {} are unique. I.e. avg of {:0.2f} words per tweet.\".format(token_count, len(vocabulary), token_count/df_a.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Model\n",
    "#### Tokenize tweets  |  Turn into Index sequences  |  Pad sequences to max_length  |  (optional) Use word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GloVe Twitter word-embedding how to:\n",
    "1. Read embeddings and build a [word -> vector] dictionary\n",
    "2. Use tokenizer [word -> index] dictionary to iterate through all words in vocab\n",
    "3. For each word in vocab (iterate over [word -> index] tokenizer dictionary): \n",
    "    - Attempt to get word's embedding vector from GloVe Twitter [word -> vector]\n",
    "    - If word is in embedding's vocabulary: add the vector to the embedding matrix at the right index\n",
    "    - If not, do not do anything (respective vector will be [0, 0, 0, ..., 0]\n",
    "\n",
    "Embedding matrix will be of size (nb_words + 1, embedding_dim), this will be loaded into the embeddings layer later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of unique tokens in tokenizer: 14737\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "##   BUILD VOCABULARY FROM CORPUS   ################################################################\n",
    "####################################################################################################\n",
    "max_seq_len    = 50\n",
    "max_features   = 30000\n",
    "\n",
    "# Tokenize all tweets\n",
    "tokenizer = Tokenizer(lower=True, filters='', split=' ')\n",
    "X_all = list(X) + list(X_trial)\n",
    "tokenizer.fit_on_texts(X_all)\n",
    "print(f\"Num of unique tokens in tokenizer: {len(tokenizer.word_index)}\")\n",
    "\n",
    "# Get sequences for each dataset\n",
    "sequences       = tokenizer.texts_to_sequences(X)\n",
    "sequences_trial = tokenizer.texts_to_sequences(X_trial)\n",
    "\n",
    "# Pad sequences\n",
    "X       = pad_sequences(sequences, maxlen = max_seq_len)\n",
    "X_trial = pad_sequences(sequences_trial, maxlen = max_seq_len)\n",
    "\n",
    "# Reshape labels\n",
    "y       = y.reshape(-1,1)\n",
    "y_trial = y_trial.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embedding matrix (14738, 100) - Done!\n",
      "This vocabulary has 14737 unique tokens of which 12940 are in the embeddings and 1797 are not\n",
      "Words not in Glove: 1797\n"
     ]
    }
   ],
   "source": [
    "if use_pretrained_embeddings:\n",
    "    # Build Embedding Matrix\n",
    "    n_words_in_glove = 0\n",
    "    n_words_not_in_glove = 0\n",
    "    words_in_glove = []\n",
    "    words_not_in_glove = []\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "    print(f\"Building embedding matrix {embedding_matrix.shape}\", end=\"\")\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            n_words_in_glove += 1\n",
    "            words_in_glove.append(word)\n",
    "        else:\n",
    "            n_words_not_in_glove += 1\n",
    "            words_not_in_glove.append(word)\n",
    "    print(\" - Done!\")\n",
    "    print(\"This vocabulary has {} unique tokens of which {} are in the embeddings and {} are not\".format(len(word_index), n_words_in_glove, n_words_not_in_glove))\n",
    "    print(f\"Words not in Glove: {len(words_not_in_glove)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BUILD MODEL & TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define RNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_Bi_GRU_LSTM_CN_model(lr=0.001, lr_decay=0.01, recurrent_units=0, dropout=0.0):\n",
    "    # Model architecture\n",
    "    inputs = Input(shape = (max_seq_len,), name=\"Input\")\n",
    "    \n",
    "    emb = Embedding(nb_words+1, embed_size, trainable=train_embeddings, name=\"WordEmbeddings\")(inputs)\n",
    "    emb = SpatialDropout1D(dropout)(emb)\n",
    "\n",
    "    gru_out  = Bidirectional(CuDNNGRU(RECURRENT_UNITS, return_sequences = True), name=\"Bi_GRU\")(emb)\n",
    "    gru_out  = Conv1D(32, 4, activation='relu', padding='valid', kernel_initializer='he_uniform')(gru_out)\n",
    "\n",
    "    lstm_out = Bidirectional(CuDNNLSTM(RECURRENT_UNITS, return_sequences = True), name=\"Bi_LSTM\")(emb)\n",
    "    lstm_out = Conv1D(32, 4, activation='relu', padding='valid', kernel_initializer='he_uniform')(lstm_out)\n",
    "\n",
    "    avg_pool1 = GlobalAveragePooling1D(name=\"GlobalAVGPooling_GRU\")(gru_out)\n",
    "    max_pool1 = GlobalMaxPooling1D(name=\"GlobalMAXPooling_GRU\")(gru_out)\n",
    "\n",
    "    avg_pool2 = GlobalAveragePooling1D(name=\"GlobalAVGPooling_LSTM\")(lstm_out)\n",
    "    max_pool2 = GlobalMaxPooling1D(name=\"GlobalMAXPooling_LSTM\")(lstm_out) \n",
    "\n",
    "    x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "\n",
    "    outputs = Dense(1, activation='sigmoid', name=\"Output\")(x)\n",
    "    \n",
    "    model = Model(inputs,outputs)\n",
    "    \n",
    "    return model, 1\n",
    "\n",
    "def build_LSTM():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words+1, embed_size, input_length=max_seq_len, trainable=train_embeddings, name=\"Embeddings\"))\n",
    "    model.add(SpatialDropout1D(DROPOUT))\n",
    "    model.add(Bidirectional(LSTM(RECURRENT_UNITS)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model, 0\n",
    "\n",
    "def build_CNN_LSTM():\n",
    "    EMBEDDING_DIM = embed_size\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words+1, EMBEDDING_DIM, input_length=max_seq_len, trainable=train_embeddings, name=\"Embeddings\"))\n",
    "    model.add(SpatialDropout1D(DROPOUT))\n",
    "    model.add(Conv1D(64, 4, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(Bidirectional(LSTM(RECURRENT_UNITS, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model, 0\n",
    "\n",
    "def build_LSTM_CNN():\n",
    "    EMBEDDING_DIM = embed_size\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words+1, EMBEDDING_DIM, input_length=max_seq_len, trainable=train_embeddings, name=\"Embeddings\"))\n",
    "#     model.add(SpatialDropout1D(DROPOUT))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Bidirectional(CuDNNLSTM(RECURRENT_UNITS, return_sequences=True)))\n",
    "#     model.add(Bidirectional(LSTM(EMBEDDING_DIM, return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)))\n",
    "    model.add(Conv1D(64, kernel_size=2, activation='relu', padding='valid', kernel_initializer='he_uniform'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004, 0, 20, 32, 100, 0.4, 0.0]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Embeddings (Embedding)       (None, 50, 100)           1473800   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               160800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 1,634,801\n",
      "Trainable params: 161,001\n",
      "Non-trainable params: 1,473,800\n",
      "_________________________________________________________________\n",
      "Class weights (to address dataset imbalance):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0.748868778280543, 1: 1.5045454545454546}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13240 samples, validate on 320 samples\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# SET HYPERPARAMETERS -------------------------------------------------------------------------------------------\n",
    "LR               = 0.004\n",
    "LR_DECAY         = 0\n",
    "EPOCHS           = 20\n",
    "BATCH_SIZE       = 32\n",
    "EMBEDDING_DIM    = embed_size\n",
    "DROPOUT          = 0.4         # Connection drop ratio for CNN to LSTM dropout\n",
    "LSTM_DROPOUT     = 0.0         # Connection drop ratio for gate-specific dropout\n",
    "BIDIRECTIONAL    = True\n",
    "RECURRENT_UNITS  = 100\n",
    "train_embeddings = not use_pretrained_embeddings\n",
    "print([LR, LR_DECAY, EPOCHS, BATCH_SIZE, EMBEDDING_DIM, DROPOUT, LSTM_DROPOUT])\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# BUILD MODEL --------------------------------------------------------------------------------------------------\n",
    "# - Can select which architecture to use (simple LSTM works pretty well)\n",
    "model, embed_idx = build_LSTM()\n",
    "# model, embed_idx = build_CNN_LSTM()\n",
    "# model, embed_idx = build_LSTM_CNN()\n",
    "# model, embed_idx = build_Bi_GRU_LSTM_CN_model(LR, LR_DECAY, RECURRENT_UNITS, DROPOUT)\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# OPTIMIZER | COMPILE | EMBEDDINGS -----------------------------------------------------------------------------\n",
    "optim = optimizers.Adam(lr=LR, decay=LR_DECAY)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "if use_pretrained_embeddings:\n",
    "    model.layers[embed_idx].set_weights([embedding_matrix])\n",
    "model.summary()\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "# X_train, X_val, y_train, y_val = (X, y, X_trial, Y_trial)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Can change right hand side to (X, y, X_trial, y_trial) or (X_train, Y_train, X_val, Y_val)\n",
    "X_train, y_train, X_val, y_val = (X, y, X_trial, y_trial)\n",
    "\n",
    "class_weights = sklearn.utils.class_weight.compute_class_weight('balanced', np.unique(y_train), y_train.reshape(-1))\n",
    "weights_dict = dict()\n",
    "for i, weight in enumerate(class_weights):\n",
    "    weights_dict[i] = weight\n",
    "print(\"Class weights (to address dataset imbalance):\")\n",
    "display(weights_dict)\n",
    "\n",
    "\n",
    "# FIT THE MODEL ------------------------------------------------------------------------------------------------\n",
    "auc_f1     = ROC_F1(validation_data=(X_val, y_val), training_data=(X_train, y_train), interval=1)\n",
    "earlystop  = EarlyStopping(monitor='val_loss', patience=15, verbose=1, mode='auto', restore_best_weights=True)\n",
    "filepath   = \"weights-improvement-{epoch:02d}-{val_acc:.5f}-{val_loss:.5f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, save_best_only=True, monitor='val_loss', verbose=1, mode='min')\n",
    "\n",
    "train_history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                          verbose=1, class_weight=class_weights, callbacks=[earlystop, checkpoint, auc_f1])\n",
    "#---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate [Training History, Classification Report + Metrics]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "height = 3.5;    width = height*4\n",
    "n_epochs = 40\n",
    "# n_epochs = len(train_history.history['loss'])\n",
    "\n",
    "# # Plot Loss\n",
    "# plt.figure(figsize=(width,height))\n",
    "# plt.plot(train_history.history['loss'], label=\"Train Loss\")\n",
    "# plt.plot(train_history.history['val_loss'], label=\"Validation Loss\")\n",
    "# plt.xlim([0,n_epochs-1]); plt.xticks(list(range(n_epochs)));   plt.grid(True);   plt.legend()\n",
    "# plt.title(\"Loss (Binary Cross-entropy)\", fontsize=15)\n",
    "# plt.show()\n",
    "\n",
    "# # Plot accuracy\n",
    "# plt.figure(figsize=(width,height))\n",
    "# plt.plot(train_history.history['acc'], label=\"Train Accuracy\")\n",
    "# plt.plot(train_history.history['val_acc'], label=\"Validation Accuracy\")\n",
    "# plt.xlim([0,n_epochs-1]); plt.xticks(list(range(n_epochs)));   plt.grid(True);   plt.legend()\n",
    "# plt.title(\"Accuracy\", fontsize=15)\n",
    "# plt.show()\n",
    "\n",
    "# Plot F1\n",
    "plt.figure(figsize=(width,height))\n",
    "plt.plot(auc_f1.f1s_train, label=\"Train F1\")\n",
    "plt.plot(auc_f1.f1s_val, label=\"Validation F1\")\n",
    "plt.xlim([0,n_epochs-1]); plt.xticks(list(range(n_epochs)));   plt.grid(True);   plt.legend()\n",
    "plt.title(\"F1-score\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "# # Plot ROC AUC\n",
    "plt.figure(figsize=(width,height))\n",
    "plt.plot(auc_f1.aucs_train, label=\"Train ROC AUC\")\n",
    "plt.plot(auc_f1.aucs_val, label=\"Validation ROC AUC\")\n",
    "plt.xlim([0,n_epochs-1]); plt.xticks(list(range(n_epochs)));   plt.grid(True);   plt.legend()\n",
    "plt.legend()\n",
    "plt.title(\"ROC AUC\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "##### Confusion matrix & Classication Report ###################################################\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# def plot_confusion_matrix(cm, classes,\n",
    "#                           normalize=False,\n",
    "#                           title='Confusion matrix',\n",
    "#                           cmap=plt.cm.winter):\n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title, fontsize=30)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, fontsize=20)\n",
    "#     plt.yticks(tick_marks, classes, fontsize=20)\n",
    "    \n",
    "#     fmt = '.2f' if normalize else 'd'\n",
    "#     thresh = cm.max() / 2.\n",
    "\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "#                  color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label', fontsize=30)\n",
    "#     plt.xlabel('Predicted label', fontsize=30)\n",
    "\n",
    "#     return plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "X_eval = X_trial\n",
    "y_eval = y_trial\n",
    "\n",
    "y_pred = model.predict(X_eval)\n",
    "y_pred = np.round(y_pred)\n",
    "print(\"Validation Accuracy: {:0.2f}%\".format(np.sum(y_eval==y_pred)/y_eval.shape[0]*100))\n",
    "\n",
    "cm = confusion_matrix(y_eval, y_pred)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm, classes=['NOT-OFFENSIVE','OFFENSIVE'], normalize=True, title='Confusion matrix')\n",
    "plt.show()\n",
    "# print(cm)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model_A.h5\")\n",
    "params = dict(remove_USER_URL=True,\n",
    "              remove_stopwords=False,\n",
    "              remove_HTMLentities=True,\n",
    "              remove_punctuation=True,\n",
    "              appostrophe_handling=True,\n",
    "              lemmatize=True)\n",
    "\n",
    "df_test = pd.read_csv('../OffensEval_data/test/testset-taska.tsv',sep='\\t')\n",
    "ID = df_test['id']\n",
    "\n",
    "X_test = df_test['tweet'].apply(lambda x: process_tweet(x, **params, trial=False)).values\n",
    "\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(sequences_test, maxlen = max_seq_len)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "OFF = y_pred\n",
    "NOT = 1-y_pred\n",
    "Y_pred = np.hstack((NOT, OFF))\n",
    "\n",
    "classes = np.argmax(Y_pred, axis=1).reshape(-1,1)\n",
    "\n",
    "df_submission = pd.DataFrame()\n",
    "df_submission['id'] = ID\n",
    "df_submission['label'] = classes\n",
    "df_submission['label'] = df_submission['label'].replace({0: 'NOT', 1: 'OFF'})\n",
    "\n",
    "df_submission.head()\n",
    "df_submission.to_csv(\"submission_a/test_a_submission.csv\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO USE TWITTER GLOVE EMBEDDINGS ##########################\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "# https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "\n",
    "# PRE-PROCESS\n",
    "import re\n",
    "def glove_preprocess(text):\n",
    "    \"\"\"\n",
    "    adapted from https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n",
    "    \"\"\"\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = \"[8:=;]\"\n",
    "    nose = \"['`\\-]?\"\n",
    "    text = re.sub(r'http\\S+', '<URL> ', text)\n",
    "    text = re.sub(r'@\\S+', '<USER>', text)\n",
    "    text = re.sub(\"www.* \", \"<URL>\", text)\n",
    "    text = re.sub(\"\\[\\[User(.*)\\|\", '<USER>', text)\n",
    "    text = re.sub(\"<3\", r'<HEART> ', text)\n",
    "    text = re.sub(\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<NUMBER>\", text)\n",
    "    text = re.sub(eyes + nose + \"[Dd)]\", '<SMILE>', text)\n",
    "    text = re.sub(\"[(d]\" + nose + eyes, '<SMILE>', text)\n",
    "    text = re.sub(eyes + nose + \"p\", '<LOLFACE>', text)\n",
    "    text = re.sub(eyes + nose + \"\\(\", '<SADFACE>', text)\n",
    "    text = re.sub(\"\\)\" + nose + eyes, '<SADFACE>', text)\n",
    "    text = re.sub(eyes + nose + \"[/|l*]\", '<NEUTRALFACE>', text)\n",
    "    text = re.sub(\"/\", \" / \", text)\n",
    "    text = re.sub(\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<NUMBER>\", text)\n",
    "    text = re.sub(\"([!]){2,}\", \"! <REPEAT>\", text)\n",
    "    text = re.sub(\"([?]){2,}\", \"? <REPEAT>\", text)\n",
    "    text = re.sub(\"([.]){2,}\", \". <REPEAT>\", text)\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    text = pattern.sub(r\"\\1\" + \" <ELONG>\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"@handermoren you are an absolute moron ???????? idiot 6 :( :p :) https://google.es\"\n",
    "print(text)\n",
    "text = glove_preprocess(text)\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "482px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
